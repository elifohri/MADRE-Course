{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "a580ae63",
   "metadata": {},
   "source": [
    "# TP 2: Regression and Cross Validation for Network Data Analysis"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9319b526",
   "metadata": {},
   "source": [
    "## üìù Exercise 1: Linear Regression with Training and Test Data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b292f99c",
   "metadata": {},
   "source": [
    "In this exercise, you will revisit simple linear regression exercise from last week but this time with a focus on model training and evaluation using a train/test split."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "de0b64f7",
   "metadata": {},
   "source": [
    "### Step 1: Import Required Libraries\n",
    "\n",
    "First, let's import the necessary libraries for data manipulation, visualization and regression modeling."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9f6467d2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import required libraries\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.metrics import mean_squared_error, r2_score\n",
    "\n",
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "99ecd0f4",
   "metadata": {},
   "source": [
    "### Step 2: Generate the Data\n",
    "\n",
    "Before training a model, we need data that represents the problem we want to solve. In this lab, we use a predefined function `genSample()` to simulate synthetic data that mimics real-world behavior (e.g network-related variables).\n",
    "\n",
    "* a) Use the provided `genSample()` function from the previous lab session to generate your dataset.\n",
    "\n",
    "    - Choose appropriate parameters: number of samples, intercept, slope and noise level\n",
    "    - Store the output into variables for input features and target labels"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3712db0e",
   "metadata": {},
   "source": [
    "#### Answer:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "39c2d168",
   "metadata": {},
   "outputs": [],
   "source": [
    "## TODO:"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2038de6a",
   "metadata": {},
   "source": [
    "### Step 3: Split the Dataset\n",
    "\n",
    "When building predictive models, we want to know not only how well the model fits the **data it sees**, but also how well it will perform on **unseen data**. \n",
    "\n",
    "To do this, we split the dataset into two parts:\n",
    "- A **training set** to learn from\n",
    "- A **test set** to evaluate generalization\n",
    "\n",
    "This helps prevent **overfitting** and provides a more realistic estimate of how your model will perform in production or deployment.\n",
    "\n",
    "* b) Split the dataset into a **training set** and a **test set**.\n",
    "\n",
    "    - Use `train_test_split()` from `sklearn.model_selection`\n",
    "    - Set a test size of 20% and use a random seed for reproducibility"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "429b60f8",
   "metadata": {},
   "source": [
    "#### Answer:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "8a915e62",
   "metadata": {},
   "outputs": [],
   "source": [
    "## TODO:"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cc517c78",
   "metadata": {},
   "source": [
    "### Step 4: Train the Model\n",
    "\n",
    "Once the data is split, the next step is to fit a linear regression model using only the training set. This means the model will learn the best-fitting line by minimizing the error between the predicted and actual values on the training data. \n",
    "\n",
    "* c) Create and train a linear regression model using only the **training data**.\n",
    "\n",
    "    - Use `LinearRegression()` from `sklearn.linear_model`\n",
    "    - Fit the model using `.fit(...)` with your training inputs and outputs"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aee6eb32",
   "metadata": {},
   "source": [
    "#### Answer:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "0df2ce48",
   "metadata": {},
   "outputs": [],
   "source": [
    "## TODO:"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4d827507",
   "metadata": {},
   "source": [
    "### Step 5: Make Predictions\n",
    "\n",
    "Once the model is trained, you can use it to predict outcomes for the test set, which simulates new, unseen data. This step evaluates how well the model generalizes.\n",
    "\n",
    "* d) Use the trained model to predict the outputs for your test set.\n",
    "\n",
    "    - Use the `.predict(...)` method\n",
    "    - Store the predicted values for comparison with the actual test labels"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "731b2d21",
   "metadata": {},
   "source": [
    "#### Answer:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "7df621c9",
   "metadata": {},
   "outputs": [],
   "source": [
    "## TODO:"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1ce11647",
   "metadata": {},
   "source": [
    "### Step 6: Evaluate the Model\n",
    "\n",
    "To measure how well the model performs, we use quantitative evaluation metrics. Two common ones are:\n",
    "\n",
    "- **Mean Squared Error (MSE)**: Measures the average squared difference between predicted and actual values\n",
    "- **R¬≤ Score**: Indicates how much of the variance in the output variable is explained by the model (closer to 1 is better)\n",
    "\n",
    "A low MSE and high R¬≤ generally indicate a good model fit.\n",
    "\n",
    "* e) Evaluate the accuracy of your predictions using the test data.\n",
    "\n",
    "    - Compute **Mean Squared Error (MSE)** using `mean_squared_error(...)`\n",
    "    - Compute **R¬≤ Score** using `r2_score(...)`\n",
    "\n",
    "* f) Comment on the results.\n",
    "    - Do the MSE and R¬≤ scores indicate a good fit on the test set?\n",
    "    - What might cause a model to perform poorly on test data, even if it performs well on the training data?\n",
    "    - Is there any indication of overfitting or underfitting?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9c4e8250",
   "metadata": {},
   "source": [
    "#### Answer:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "30650e36",
   "metadata": {},
   "outputs": [],
   "source": [
    "## TODO:"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b61c5a17",
   "metadata": {},
   "source": [
    "### Step 7: Visualize the Results\n",
    "\n",
    "Visualizing your model‚Äôs predictions helps you understand how well it fits the data. A well-fitted model should have its regression line align closely with the true data points in the test set.\n",
    "\n",
    "* g) Plot the following on a graph:\n",
    "\n",
    "    - The original test data (as a scatter plot)\n",
    "    - The model‚Äôs predicted outputs (regression line)\n",
    "    - Add axis labels, a title, and a legend."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a28ea855",
   "metadata": {},
   "source": [
    "#### Answer:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "52971f6e",
   "metadata": {},
   "outputs": [],
   "source": [
    "## TODO:"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8c609782",
   "metadata": {},
   "source": [
    "## üìù Exercise 2: Polynomial Regression"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "574a0550",
   "metadata": {},
   "source": [
    "In this exercise, you will explore **polynomial regression** as an extension of linear models. Polynomial regression allows us to fit more complex, non-linear relationships by using polynomial features derived from a single input variable."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "731ab1c5",
   "metadata": {},
   "source": [
    "We are going to generate synthetic data based on a non-linear, degree-3 polynomial function: $y = 4 + 2x + 0.5x^2 - 0.07x^3 + \\epsilon$.\n",
    "\n",
    "* The input $x$ is sampled uniformly from the interval [0, 10].\n",
    "* The noise $\\epsilon$ is sampled from a normal distribution with mean 0 and standard deviation $\\sigma = 5$.\n",
    "\n",
    "This is a cubic polynomial with Gaussian noise added. This equation represents the hidden ground truth that we will later try to discover using regression.\n",
    "\n",
    "You will then fit regression models of varying polynomial degrees (from 1 to 16) and evaluate how well they approximate the true curve.\n",
    "\n",
    "The general form of a polynomial regression model with degree $\\ell$ is: $\\hat{y} = \\hat{\\beta}_0 + \\hat{\\beta}_1x + \\hat{\\beta}_2x^2 +\\ldots + \\hat{\\beta}_{\\ell}x^{\\ell}$.\n",
    "\n",
    "Although you are still working with a single input variable $x$, transforming it into $x^2, x^3, \\ldots$ means the model effectively operates in a **higher-dimensional space**.\n",
    "\n",
    "Your task is to evaluate how different polynomial degrees affect the model's performance and generalization."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9483ea86",
   "metadata": {},
   "source": [
    "### Step 1: Generate Synthetic Data\n",
    "\n",
    "Using the following parameters:\n",
    "- $\\sigma = 5$ (standard deviation of the noise)\n",
    "- $n = 200$ data points\n",
    "- $x \\sim \\text{Uniform}(0, 10)$\n",
    "\n",
    "We generate data for:\n",
    "  - Feature: $x$\n",
    "  - Derived features: $x^2, x^3$\n",
    "  - Noise: $\\varepsilon \\sim \\mathcal{N}(0, \\sigma^2)$\n",
    "  - Target: $y = 4 + 2x + 0.5x^2 - 0.07x^3 + \\varepsilon$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "786f3c02",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Parameters\n",
    "n = 200\n",
    "b0 = 4\n",
    "b1 = np.array([2,0.5,-0.07])\n",
    "mue, sigmae = 0, 5\n",
    "xl, xh = 0, 10\n",
    "\n",
    "# Set random seed for reproducibility\n",
    "np.random.seed(199)\n",
    "Er = np.random.normal(mue, sigmae, n)\n",
    "np.random.seed(199)\n",
    "\n",
    "# Generate synthetic x values\n",
    "x0 = np.random.uniform(xl,xh,n)\n",
    "x = np.array([x0])\n",
    "x = np.append(x,np.array([x0**2]),axis=0)\n",
    "x = np.append(x,np.array([x0**3]),axis=0)\n",
    "\n",
    "\n",
    "# Generate true y values using the polynomial plus noise\n",
    "y = b0 + b1[0]*x[0]+ b1[1]*x[1]+ b1[2]*x[2]+Er"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0508445f",
   "metadata": {},
   "source": [
    "Then we store the data in data frames and prepare it for polynomial regression. From this point on you don't know how this data was produced, you just have the data on x and y as a data frame and are asked to do a regression."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "60a9bcaf",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataSynth = {'x': x[0],'y': y}\n",
    "df = pd.DataFrame(data=dataSynth)\n",
    "df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ca6f063e",
   "metadata": {},
   "source": [
    "### Step 2: Split the Dataset\n",
    "\n",
    "We have to split the data into training set and test set.\n",
    "\n",
    "* a) Split the data into a **training set** and a **test set** using an 80/20 ratio.\n",
    "\n",
    "    - Use `train_test_split()` from `sklearn.model_selection`\n",
    "    - Set a random seed for reproducibility"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "05f689f2",
   "metadata": {},
   "source": [
    "#### Answer:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c8bde054",
   "metadata": {},
   "outputs": [],
   "source": [
    "## TODO:"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d239374f",
   "metadata": {},
   "source": [
    "### Step 3: Explore the Effect of Polynomial Degree\n",
    "\n",
    "In this part, you will investigate how changing the **degree ‚Ñì of the polynomial** affects the model‚Äôs ability to fit and generalize.\n",
    "\n",
    "Polynomial regression is still linear regression but applied to transformed features: $\\hat{y} = \\hat{\\beta}_0 + \\hat{\\beta}_1 x + \\hat{\\beta}_2 x^2 + \\dots + \\hat{\\beta}_\\ell x^\\ell$\n",
    "\n",
    "* b) Fit a regression model to the training data. \n",
    "* c) Predict the output on the test set.\n",
    "* d) Plot the resulting regression curve along with the original data points\n",
    "* e) Print the MSE and R¬≤ score for the test set\n",
    "* f) Does the degree-3 polynomial fit the shape of the true curve well? Comment on R¬≤ score?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1a5f1987",
   "metadata": {},
   "source": [
    "#### Answer:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4e32cfe3",
   "metadata": {},
   "outputs": [],
   "source": [
    "## TODO:"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ab3f4d02",
   "metadata": {},
   "source": [
    "### Step 4: Compare Models with Different Polynomial Degrees\n",
    "\n",
    "In this part, you will systematically evaluate how the degree of the polynomial affects your model‚Äôs performance. \n",
    "\n",
    "* g) Now repeat the entire procedure for the following degrees: $\\ell = 1,\\ 2,\\ 3,\\ 6,\\ 9,\\ 16$\n",
    "\n",
    "  For each degree ‚Ñì:\n",
    "  - **Hint**: You can transform the input using `PolynomialFeatures(degree=‚Ñì)`\n",
    "  - Train the model on the training data\n",
    "  - Predict on both training and test sets\n",
    "  - Compute and store:\n",
    "    - **Mean Squared Error (MSE)** for training and test sets\n",
    "    - **R¬≤ Score** for training and test sets\n",
    "  - Store all these results so you can visualize the trends across different polynomial degrees."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2b94f0b1",
   "metadata": {},
   "source": [
    "#### Answer:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "084d3c45",
   "metadata": {},
   "outputs": [],
   "source": [
    "## TODO:"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "619e1df6",
   "metadata": {},
   "source": [
    "### Step 5: Visualize and Analyze the Results\n",
    "\n",
    "Now use the collected results to analyze model performance.\n",
    "\n",
    "* h) Plot the $MSE$ value vs. $\\ell$ for the training dataset. \n",
    "* i) Plot the $MSE$ value vs. $\\ell$ for the test dataset. \n",
    "* j) What do you observe?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6ad85df8",
   "metadata": {},
   "source": [
    "#### Answer:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7dd32676",
   "metadata": {},
   "outputs": [],
   "source": [
    "## TODO:"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fd1eee9c",
   "metadata": {},
   "source": [
    "* k) Plot the $R^2$ value vs. $\\ell$ for the training dataset. \n",
    "* l) Plot the $R^2$ value vs. $\\ell$ for the test dataset. \n",
    "* m) What do you observe?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e5b2eb07",
   "metadata": {},
   "source": [
    "#### Answer:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a3b5ceec",
   "metadata": {},
   "outputs": [],
   "source": [
    "## TODO:"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0a089556",
   "metadata": {},
   "source": [
    "## üìù Exercise 3: Cross Validation"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "895a2f7d",
   "metadata": {},
   "source": [
    "In this exercise, you will explore different **cross-validation (CV)** strategies for evaluating a model‚Äôs generalization ability on small datasets.\n",
    "\n",
    "Cross-validation helps estimate how well a model trained on one subset of data performs on unseen data."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9d2a8154",
   "metadata": {},
   "source": [
    "### Step 1: Define a Toy Dataset\n",
    "\n",
    "We will work with a very small dataset, defined as: $D_6 = \\left\\{(1,3),\\ (2,4),\\ (3,8),\\ (4,9),\\ (5,12),\\ (7,14) \\right\\}$\n",
    "\n",
    "Each pair $(x_i, y_i)$ represents an input‚Äìoutput example. \n",
    "\n",
    "* a) Store this dataset as `NumPy` arrays or a `Pandas` DataFrame."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "451b490b",
   "metadata": {},
   "source": [
    "#### Answer:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d42c5e07",
   "metadata": {},
   "outputs": [],
   "source": [
    "## TODO:"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1d5d66b7",
   "metadata": {},
   "source": [
    "### Step 2: Cross-Validation Splits\n",
    "\n",
    "In this part, you will explore how different cross-validation strategies divide the dataset into **training and test sets**.\n",
    "\n",
    "For each method below, print out all train-test index splits and understand how the dataset is divided.\n",
    "\n",
    "For each method, clearly print:\n",
    "- The train indices\n",
    "- The test indices\n",
    "\n",
    "* b) Leave-One-Out Cross Validation (LOOCV):\n",
    "   - Use: `LeaveOneOut()` from `sklearn.model_selection`"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cb2c5458",
   "metadata": {},
   "source": [
    "#### Answer:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ca447efb",
   "metadata": {},
   "outputs": [],
   "source": [
    "## TODO:"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c678c27d",
   "metadata": {},
   "source": [
    "* c) 3-Fold Cross Validation:\n",
    "   - Use: `KFold(n_splits=3)` from `sklearn.model_selection`"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4c7c6651",
   "metadata": {},
   "source": [
    "#### Answer:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "05b1285c",
   "metadata": {},
   "outputs": [],
   "source": [
    "## TODO:"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "44c96100",
   "metadata": {},
   "source": [
    "* d) Bootstrap Method:\n",
    "   - Use: `Bootstrap(n_resamples=5)` from `sklearn.utils`"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "97f0fd9a",
   "metadata": {},
   "source": [
    "#### Answer:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "655415d5",
   "metadata": {},
   "outputs": [],
   "source": [
    "## TODO:"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "073b3dc3",
   "metadata": {},
   "source": [
    "### Step 3: Apply Cross-Validation for Model Evaluation\n",
    "\n",
    "Now you will fit a linear regression model and compute the average test MSE using a specific CV strategy.\n",
    "\n",
    "* e) Use 2-Fold Cross Validation to estimate the model‚Äôs average test error.\n",
    "   - Use `cross_validate(...)` or `cross_val_score(...)` from `sklearn.model_selection`\n",
    "   - Use `KFold(n_splits=2)`\n",
    "\n",
    "Print the MSE for each fold, and compute the average test error across the two folds."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e32faf07",
   "metadata": {},
   "source": [
    "#### Answer:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "f3ff3399",
   "metadata": {},
   "outputs": [],
   "source": [
    "## TODO:"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f7584019",
   "metadata": {},
   "source": [
    "### Step 4: Apply the same method to the data set below for the **TV-sales** pair:\n",
    "\n",
    "The dataset \"Advertising.csv\" can be downloaded from: http://faculty.marshall.usc.edu/gareth-james/ISL/data.html\n",
    "\n",
    "If the previous link doesn't work, please follow this backup link: https://raw.githubusercontent.com/justmarkham/scikit-learn-videos/master/data/Advertising.csv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ea9d49a2",
   "metadata": {},
   "outputs": [],
   "source": [
    "directory = #your directory\n",
    "prefix = \"Advertising.csv\"\n",
    "filename1 = directory+prefix\n",
    "dataAd = np.loadtxt(filename1, delimiter=\",\",skiprows=1,usecols=[1,2,3,4])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c857278a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# pandas\n",
    "pdAd = pd.DataFrame(dataAd, columns=[\"TV\",\"radio\",\"newspaper\",\"sales\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "42eeecd6",
   "metadata": {},
   "outputs": [],
   "source": [
    "TV = pdAd.iloc[:,0].values\n",
    "#radio = pdAd.iloc[:,1].values\n",
    "#news = pdAd.iloc[:,2].values\n",
    "sales = pdAd.iloc[:,3].values"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cd8cdc50",
   "metadata": {},
   "source": [
    "#### **Answer:**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ea45432b",
   "metadata": {},
   "outputs": [],
   "source": [
    "## TODO:"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
