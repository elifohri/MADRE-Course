{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "f5490e80",
   "metadata": {},
   "source": [
    "# TP 6: Neural Networks"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a645c451",
   "metadata": {},
   "source": [
    "## Quick Recap: Sotochastic Gradient Descent (SGD)\n",
    "\n",
    "Gradient Descent is an iterative optimization algorithm that finds the minimum of a loss function by taking steps in the direction of the steepest descent (the negative gradient). \n",
    "\n",
    "The update rule is:\n",
    "\n",
    "$$\\theta = \\theta - \\eta \\nabla L(\\theta)$$\n",
    "\n",
    "where:\n",
    "- $\\theta$ represents the model parameters\n",
    "- $\\eta$ is the **learning rate** (step size)\n",
    "- $\\nabla L(\\theta)$ is the gradient of the loss function\n",
    "\n",
    "**Stochastic Gradient Descent** updates parameters using only a small random subset of data at each iteration, rather than the entire dataset. This makes training:\n",
    "\n",
    "- Faster (fewer data points to process)\n",
    "- More memory-efficient"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ba568391",
   "metadata": {},
   "source": [
    "## üìù Exercise 1: Stochastic Gradient Descent for Linear Regression"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "81d4c840",
   "metadata": {},
   "source": [
    "In this exercise, we will implement the **Stochastic Gradient Descent (SGD)** algorithm from scratch to solve a Linear Regression problem."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "71e3a7d4",
   "metadata": {},
   "source": [
    "### The Problem Setup:\n",
    "\n",
    "We want to fit a **linear regression model** to synthetic data: $$y = \\beta_0 + \\beta_1 x_1 + \\beta_2 x_2 + \\epsilon$$\n",
    "\n",
    "where:\n",
    "- $\\beta_0$ is the intercept\n",
    "- $\\beta_1, \\beta_2$ are the slope coefficients\n",
    "- $\\epsilon$ is Gaussian noise\n",
    "\n",
    "We'll generate training data from known parameters, then use SGD to recover them."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4d931275",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Import necessary libraries\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import math\n",
    "import random"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b43efc2d",
   "metadata": {},
   "source": [
    "### Part 1: Synthetic Data Generation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8d16c5e6",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Set the \"True\" Parameters\n",
    "n = 60  # Number of samples\n",
    "b0 = 5  # True intercept (what we want to recover)\n",
    "b1 = np.array([2, -3])  # True coefficients (what we want to recover)\n",
    "\n",
    "## Add realistic noise\n",
    "mue = 0  # Mean of noise\n",
    "sigmae = 5  # Standard deviation of noise\n",
    "\n",
    "## Range of input values\n",
    "xl, xh = 0, 10  # x ranges from 0 to 10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9d41f71d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def genSample(n,b0,b1,sigmae,xLow,xHigh,seedit=199,size=1):\n",
    "    if type(seedit)==int:\n",
    "        np.random.seed(seedit)\n",
    "        Er = np.random.normal(mue, sigmae, n)\n",
    "        x = []\n",
    "        for k in range(size):\n",
    "            np.random.seed(seedit+k)\n",
    "            x.append(np.random.uniform(xl,xh,n))\n",
    "    else:\n",
    "        np.random.seed()\n",
    "        Er = np.random.normal(mue, sigmae, n)\n",
    "        x = []\n",
    "        for k in range(size):\n",
    "            np.random.seed()\n",
    "            x.append(np.random.uniform(xl,xh,n))\n",
    "    y = b0+Er\n",
    "    for k in range(size):\n",
    "        y +=b1[k]*x[k]\n",
    "    \n",
    "    ## Output\n",
    "    if size==1:\n",
    "        return (x[0], y, Er)\n",
    "    else:\n",
    "        return (x, y, Er)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aebab23e",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Use the provided `genSample()` function to create training data\n",
    "(x,y,Er) = genSample(n,b0,b1,sigmae,xl,xh,seedit=199,size=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "99bdea7b",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Organize into a DataFrame for easier inspection\n",
    "data1 = {'x1': x[0], 'x2': x[1], 'error': Er, 'y': y}\n",
    "df_slr = pd.DataFrame(data=data1)\n",
    "df_slr"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "68e03193",
   "metadata": {},
   "source": [
    "This gives us:\n",
    "- `x[0]`: First feature, 60 samples\n",
    "- `x[1]`: Second feature, 60 samples\n",
    "- `y`: Target variable for each sample\n",
    "- `Er`: Noise added to each sample"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "41bb3cbc",
   "metadata": {},
   "source": [
    "### Part 2: Implementing SGD"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "053a8391",
   "metadata": {},
   "source": [
    "In this section, we will evaluate how the performance of Stochastic Gradient Descent (SGD) changes when we adjust two critical hyperparameters: \n",
    "\n",
    "- Mini-batch Size (m)\n",
    "- Learning Rate ($\\eta$)\n",
    "\n",
    "**The Mini-batch Size (m):** This is the amount of data the model looks at before making an update.\n",
    "\n",
    "**The Learning Rate ($\\eta$):** It determines how much we change our parameters after seeing an error. If it's too large, we might skip over the best solution; if it's too small, the model will take forever to learn."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e255197c",
   "metadata": {},
   "source": [
    "First, complete the logic inside the code cell below. Look for the `##TODO` markers to implement the gradient calculation and the parameter update step. Once your code is working, you will run the SGD algorithm multiple times to find the converged values for the parameters $(b_0,b_1,b_2)$. You should test every combination of the following settings:\n",
    "\n",
    "- **Mini-batch Size (m):** m=1, m=10, m=n (where n is the full data size)\n",
    "- **Learning Rate ($\\eta$):** $\\eta$=0.04, $\\eta$=0.01, $\\eta$=0.001"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "65444939",
   "metadata": {},
   "outputs": [],
   "source": [
    "## The Complete SGD Function\n",
    "def LinReg_SGD(T, m, eta, printit=True):\n",
    "    \"\"\"\n",
    "    Stochastic Gradient Descent for Linear Regression\n",
    "    \n",
    "    Parameters:\n",
    "    -----------\n",
    "    T : int\n",
    "        Number of epochs (passes through the data)\n",
    "    m : int\n",
    "        Mini-batch size (m=1: online, m=n: batch gradient descent)\n",
    "    eta : float\n",
    "        Learning rate (larger = faster but risky, smaller = slow but stable)\n",
    "    \n",
    "    Returns:\n",
    "    --------\n",
    "    [b0List, b1List, b2List, b0_final, b1_final, b2_final]\n",
    "        Lists of parameter values at each epoch, plus final values\n",
    "    \"\"\"\n",
    "    \n",
    "    ## Initialize unknown linear parameters randomly\n",
    "    b1_init = np.random.uniform()\n",
    "    b2_init = np.random.uniform()\n",
    "    b0_init = np.random.uniform()\n",
    "    \n",
    "    ## Track parameter values through training\n",
    "    b0 = b0_init\n",
    "    b0List = [b0]\n",
    "    b1 = b1_init\n",
    "    b1List = [b1_init]\n",
    "    b2 = b2_init\n",
    "    b2List = [b2]\n",
    "    \n",
    "    ## Main training loop\n",
    "    for t in range(T):\n",
    "        ## Select a random mini-batch\n",
    "        if m < n:\n",
    "            indx = np.random.choice(np.arange(n), size=m, replace=False)\n",
    "        else:\n",
    "            indx = np.arange(n)  # Use all data if you want to use all data per iteration\n",
    "        \n",
    "        ## Use x_batch and y_batch to store the batch of data\n",
    "        y_batch = y[indx]\n",
    "        x_batch = [[x[0][i], x[1][i]] for i in indx]\n",
    "        x_batch1 = [x_batch[j][0] for j in range(m)]\n",
    "        x_batch2 = [x_batch[j][1] for j in range(m)]\n",
    "        \n",
    "        ## TODO #1: Calculate gradients\n",
    "        ## [Add code to compute grad_b0, grad_b1, grad_b2]\n",
    "        \n",
    "        ## TODO #2: Update parameters \n",
    "        ## [Add code to update b0, b1, b2]\n",
    "        \n",
    "        ## Record values for analysis\n",
    "        b0List.append(b0)\n",
    "        b1List.append(b1)\n",
    "        b2List.append(b2)\n",
    "        \n",
    "        ## Print progress\n",
    "        if t % 100 == 0 and printit == True:\n",
    "            print(f'Epoch {t:4d} | b0={b0:7.3f} | b1={b1:7.3f} | b2={b2:7.3f}')\n",
    "    \n",
    "    return [b0List, b1List, b2List, b0, b1, b2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7214bb04",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Set hyperparameters\n",
    "T = 1000 #number of epochs\n",
    "m = 10 # Batch-size m is less than or equal to n\n",
    "eta = 0.01 # Learning rate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1e0520e1",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Run SGD to estimate parameters\n",
    "results = LinReg_SGD(T, m, eta, printit=True)\n",
    "b0List, b1List, b2List, b0, b1, b2 = results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a431ee9f",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Analyze results\n",
    "print(f\"\\nTrue parameters:   b0={5.0:.3f}, b1={2.0:.3f}, b2={-3.0:.3f}\")\n",
    "print(f\"Learned parameters: b0={b0:.3f}, b1={b1:.3f}, b2={b2:.3f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f0e9396e",
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, axs = plt.subplots(3)\n",
    "fig.suptitle('Convergence of unknowns')\n",
    "\n",
    "## Plot parameter convergence 'b0'\n",
    "axs[0].plot(b0List,'r',label='b0, batch-size 10')\n",
    "axs[0].legend(loc=7)\n",
    "axs[0].grid()\n",
    "\n",
    "## Plot parameter convergence 'b1'\n",
    "axs[1].plot(b1List,'g',label='b1, batch-size 10')\n",
    "axs[1].legend(loc=7)\n",
    "axs[1].grid()\n",
    "\n",
    "## Plot parameter convergence 'b2'\n",
    "axs[2].plot(b2List,'--k',label='b2, batch-size 10')\n",
    "axs[2].legend(loc=7)\n",
    "axs[2].grid()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6ee890df",
   "metadata": {},
   "source": [
    "#### Question:\n",
    "\n",
    "- Now, run the algorithm with **different hyperparameters**, plot convergence of unknown parameters and compare results."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8f77f07e",
   "metadata": {},
   "source": [
    "#### Answer:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0bd8541b",
   "metadata": {},
   "outputs": [],
   "source": [
    "## TODO:"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "04c1c630",
   "metadata": {},
   "source": [
    "#### Question:\n",
    "\n",
    "- How does mini-batch size affect convergence?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fdfa8b8d",
   "metadata": {},
   "source": [
    "#### Answer:"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dd24828e",
   "metadata": {},
   "source": [
    " "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eedfae98",
   "metadata": {},
   "source": [
    "#### Question: \n",
    "\n",
    "- How does learning rate affect convergence?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b7e142d0",
   "metadata": {},
   "source": [
    "#### Answer:"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3b651dde",
   "metadata": {},
   "source": [
    " "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c8ebd265",
   "metadata": {},
   "source": [
    "## üìù Exercise 2: Neural Networks for Network Bandwidth Estimation"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e027677a",
   "metadata": {},
   "source": [
    "In this exercise, we move from simple linear regression to a Deep Neural Network (DNN). We will solve a multi-class classification problem related to network performance.\n",
    "\n",
    "Imagine you're building a video streaming application (like YouTube or Netflix). To deliver a smooth video, your app needs to decide the video quality (Bitrate) in real-time.\n",
    "\n",
    "- If the bitrate is too high: The network pipe gets \"clogged\" and the user sees the dreaded loading spinner (Buffering).\n",
    "\n",
    "- If the bitrate is too low: The video looks pixelated and blurry (Bad Quality).\n",
    "\n",
    "**Goal:** The goal is to estimate the available bandwidth between the server and the user so we can pick the right video quality.\n",
    "\n",
    "**Your task:** Build a neural network that analyzes network measurement histograms (8 features) and classifies the available bandwidth into one of 5 categories (ranging from Very Low to Very High)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a1d20228",
   "metadata": {},
   "source": [
    "### Dataset Overview:\n",
    "\n",
    "In this dataset, we aren't looking at raw packet data, but rather a statistical summary of how the network is behaving. A histogram of the ratio between Bits Sent and Bits Received.\n",
    "\n",
    "- **Low Ratio (Smooth Downloading):** When you stream a movie, you send a tiny \"request\" and receive a huge \"data packet.\" The ratio is very small (e.g., 0.01).\n",
    "\n",
    "- **High Ratio (Network Stress):** If the network is congested, you might be trying to send data, but the responses are slow or tiny. The ratio spikes (e.g., 50.0 or 100.0).\n",
    "\n",
    "**Input Features (X):**\n",
    "- 8 numerical features per sample\n",
    "- Each feature represents one bar in a histogram\n",
    "- Each bar counts how often a specific ratio of sent/received bits occurred during a measurement period\n",
    "- Overall the histogram shows the distribution of the ratio between bits sent vs bits received\n",
    "- Each sample represents one network measurement experiment\n",
    "\n",
    "**Target Labels (y):**\n",
    "- 5 classes: **12.5, 25, 37.5, 50, 75 Mbps**\n",
    "- These are the available bandwidth values in the testbed\n",
    "\n",
    "**Dataset sizes:**\n",
    "- Training: 1,100 samples\n",
    "- Testing: 1,000 samples\n",
    "\n",
    "**The Problem:** \n",
    "- This is a multi-class classification task. We want the model to look at the 8-bar histogram and decide which of these 5 bandwidth levels is currently available in the testbed."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3fb98d8d",
   "metadata": {},
   "source": [
    "### Part 1: Data Loading and Exploration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5a502e14",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.model_selection import train_test_split\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras import layers\n",
    "\n",
    "from imblearn.over_sampling import SMOTE\n",
    "import category_encoders as ce"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4b185f63",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the data\n",
    "X_train = pd.read_csv('X_train.csv', header=None)\n",
    "y_train = pd.read_csv('label_train.csv', header=None, names=['label'])\n",
    "X_test = pd.read_csv('X_test.csv', header=None)\n",
    "y_test = pd.read_csv('label_test.csv', header=None, names=['label'])\n",
    "\n",
    "# Convert labels to string for clarity\n",
    "y_train['label'] = y_train['label'].astype(str)\n",
    "y_test['label'] = y_test['label'].astype(str)\n",
    "\n",
    "print(\"Training set shape:\", X_train.shape)\n",
    "print(\"Test set shape:\", X_test.shape)\n",
    "print(\"\\nClass distribution in training set:\")\n",
    "print(y_train.value_counts().sort_index())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0ccf03ff",
   "metadata": {},
   "source": [
    "#### Question: \n",
    "\n",
    "- Is the dataset imbalanced? How might this affect the model?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f564f35b",
   "metadata": {},
   "source": [
    "#### Answer:"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "93fbc799",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "ce1b9c0e",
   "metadata": {},
   "source": [
    "### Part 2: Data Pre-Processing"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f175491f",
   "metadata": {},
   "source": [
    "Before training our deep neural network, we must prepare the raw network data. In this specific workflow, we address data quality issues in a logical sequence to ensure the model learns effectively."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d11975a5",
   "metadata": {},
   "source": [
    "**Class Imbalance:** We correct the class imbalance using Synthetic Minority Over-Sampling Technique (SMOTE)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "08e7972c",
   "metadata": {},
   "outputs": [],
   "source": [
    "smote = SMOTE()\n",
    "X_train, y_train = smote.fit_resample(X_train, y_train)\n",
    "\n",
    "y_train.value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1c23f0fa",
   "metadata": {},
   "source": [
    "**Standardization:** We use Standardization to give every feature a mean of 0 and a standard deviation of 1."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bd96890e",
   "metadata": {},
   "outputs": [],
   "source": [
    "scaler = StandardScaler()\n",
    "scaler.fit(X_train)\n",
    "\n",
    "X_train = scaler.transform(X_train)\n",
    "X_test = scaler.transform(X_test)\n",
    "\n",
    "print('X_train scaled\\n', X_train[0:6, :] )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5b963be4",
   "metadata": {},
   "source": [
    "**One-hot encoding:** We perform one-hot encoding to transform our categorical labels into distinct binary vectors."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3f6dcea9",
   "metadata": {},
   "outputs": [],
   "source": [
    "one_hot_encoder = ce.OneHotEncoder(cols=['label'], use_cat_names='True')\n",
    "one_hot_encoder.fit(y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "22b0db31",
   "metadata": {},
   "outputs": [],
   "source": [
    "one_hot_encoder = ce.OneHotEncoder(cols=['label'], use_cat_names='True')\n",
    "one_hot_encoder.fit(y_train)\n",
    "y_train = one_hot_encoder.transform(y_train)\n",
    "y_test  = one_hot_encoder.transform(y_test)\n",
    "\n",
    "print('y_train', y_train)\n",
    "\n",
    "num_of_classes = y_train.shape[1]\n",
    "class_names =list(y_train.columns)\n",
    "print('There are ', num_of_classes, ' classes. Their names is:', class_names)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c17e0eae",
   "metadata": {},
   "source": [
    "### Part 3: Building the Neural Network"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4fab5f65",
   "metadata": {},
   "source": [
    "A Neural Network is composed of layers. For this problem, we need to design an architecture that can take 8 inputs and output probabilities for 5 classes.\n",
    "\n",
    "1. **Input Layer:** Receives the 8 features\n",
    "\n",
    "2. **Hidden Layers:** Perform computations and learn patterns\n",
    "   - More neurons = more capacity to learn complex patterns\n",
    "   - ReLU activation = introduces non-linearity\n",
    "   \n",
    "3. **Output Layer:** Produces 5 probability scores (one per bandwidth class)\n",
    "   - Softmax activation = converts scores to probabilities (sum to 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3995d8ec",
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_simple_model(input_dim, num_classes):\n",
    "    \"\"\"\n",
    "    Build a simple neural network with 2 hidden layers.\n",
    "    \n",
    "    Architecture:\n",
    "    - Input: input_dim features\n",
    "    - Hidden Layer 1: 32 neurons, ReLU activation\n",
    "    - Hidden Layer 2: 16 neurons, ReLU activation\n",
    "    - Output: 5 neurons, Softmax activation ‚Üí Produces probability for each bandwidth class\n",
    "    \"\"\"\n",
    "    \n",
    "    model = keras.Sequential([\n",
    "        ## TODO: Add input layer with correct dimensions\n",
    "        \n",
    "        ## TODO: Add first hidden layer\n",
    "        \n",
    "        ## TODO: Add second hidden layer\n",
    "        \n",
    "        ## TODO: Add output layer for 5-class classification\n",
    "    ])\n",
    "    \n",
    "    return model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b2f55a0d",
   "metadata": {},
   "source": [
    "### Part 4: Compiling the Model\n",
    "\n",
    "Before we can start training, we must configure the learning process.\n",
    "\n",
    "To do this, we define three key components:\n",
    "\n",
    "1.  **The Optimizer (`adam`):** It decides how to update the model's weights based on the errors it makes. `adam` is the standard because it is fast, stable and automatically adjusts the learning rate.\n",
    "2.  **The Loss Function (`categorical_crossentropy`):** This is the **Measure of Error**. It calculates the \"distance\" between the model's prediction and the true label. Use `categorical_crossentropy` if your labels are One-Hot Encoded.\n",
    "3.  **The Metrics (`accuracy`):** This is the **Scoreboard**. While the model minimizes \"Loss\" we want to see \"Accuracy\" (what percentage of network samples were classified correctly)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b92696bb",
   "metadata": {},
   "outputs": [],
   "source": [
    "## TODO: Build the model\n",
    "\n",
    "\n",
    "## TODO: Compile the model with the specified settings\n",
    "\n",
    "\n",
    "## TODO: Print the summary to see the architecture"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cb7f6f27",
   "metadata": {},
   "source": [
    "#### Question:\n",
    "\n",
    "- After printing model summary, look at the `Total Trainable Parameters`. What do these parameters represent in terms of the \"weights\" and \"biases\". If this number is very high, what is the risk to our model?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2dc72ffd",
   "metadata": {},
   "source": [
    "#### Answer:"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ed57719a",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "2c0cade6",
   "metadata": {},
   "source": [
    "### Part 4: Training and Evaluation"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f4e59023",
   "metadata": {},
   "source": [
    "Now that the model is built and compiled, it is time to perform the actual training. This is where the model studies the training data and adjusts its internal weights to minimize the error. Fill in the .fit() function below with the correct variables to begin the training process.\n",
    "\n",
    "- **x:** X_train\n",
    "\n",
    "- **y:** y_train\n",
    "\n",
    "- **validation_data:** (X_val, y_val)\n",
    "\n",
    "- **epochs:** 200 (number of passes through data)\n",
    "\n",
    "- **batch_size:** 32 (samples per gradient update)\n",
    "\n",
    "- **class_weight:** class_weights (to handle imbalance!)\n",
    "\n",
    "- **verbose:** 1 (to see training progress)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6f8edf2a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Start the training process and save the results in 'history'\n",
    "history = model.fit(\n",
    "    ## TODO: Add training data, labels, epochs, batch size, and validation data\n",
    "\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "be871169",
   "metadata": {},
   "source": [
    "### Step 5: Final Evaluation & Visualizing Success\n",
    "\n",
    "Now that the training is complete, we need to determine if our model actually works."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "854a0a76",
   "metadata": {},
   "source": [
    "Evaluate the model to see how it performs on the test set. This gives us a single number for Accuracy and Loss."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2d272f6b",
   "metadata": {},
   "outputs": [],
   "source": [
    "## TODO: "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "19d87604",
   "metadata": {},
   "source": [
    "Plot the Learning Curves. Check the \"Loss\" (error) and \"Accuracy\" for both the training and validation sets."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d50cf28e",
   "metadata": {},
   "outputs": [],
   "source": [
    "## TODO:"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b2da0ac8",
   "metadata": {},
   "source": [
    "#### Question:\n",
    "\n",
    "- Does the model overfit? "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0ab96c98",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "bd540979",
   "metadata": {},
   "source": [
    "#### Question:\n",
    "\n",
    "- At what epoch does validation accuracy stop improving?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6f12de15",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
